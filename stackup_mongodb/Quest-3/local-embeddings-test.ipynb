{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Embeddings\n",
    "In this Python notebook, we will try running embedding models locally and compare their performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Llama-Index\n",
    "We'll be making use of LlamaIndex to help us download the embedding models from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you face issues running the code cell below, uncomment the 2 lines below and run this code cell to install some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLLAMA_INDEX_CACHE_DIR\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# from llama_index.embeddings import HuggingFaceEmbedding\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Uncomment the line above and comment the line below if you face an import error\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbedding\n\u001b[1;32m     10\u001b[0m embed_model \u001b[38;5;241m=\u001b[39m HuggingFaceEmbedding(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBAAI/bge-small-en-v1.5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embed_model\u001b[38;5;241m.\u001b[39mget_text_embedding(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello World!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_index'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Optional, but set llamaindex cache dir to ../cache dir here. Default is system tmp\n",
    "# This way, we can easily see downloaded artifacts\n",
    "os.environ['LLAMA_INDEX_CACHE_DIR'] = os.path.join(os.path.abspath('../'), 'cache')\n",
    "\n",
    "# from llama_index.embeddings import HuggingFaceEmbedding\n",
    "# Uncomment the line above and comment the line below if you face an import error\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "embeddings = embed_model.get_text_embedding(\"Hello World!\")\n",
    "\n",
    "print(len(embeddings))\n",
    "print(embeddings[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! As you can see from the output above, we managed to download the `BAAI/bge-small-en-v1.5` model from Hugging Face by using Llama-Index and using the downloaded model to get the vector representation of the string \"Hello World!\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Let's Try a Few Embedding Models\n",
    "As mentioned earlier in the quest, Hugging Face has many different models available for us to choose from. In this quest, we'll be using the following three models:\n",
    "\n",
    "| model name                              | overall score | model params | model size | embedding length | url                                                            |\n",
    "|-----------------------------------------|---------------|--------------|------------|------------------|----------------------------------------------------------------|\n",
    "| BAAI/bge-small-en-v1.5                  | 62.x          | 33.5 M       | 133 MB     | 384              | https://huggingface.co/BAAI/bge-small-en-v1.5                  |\n",
    "| sentence-transformers/all-mpnet-base-v2 | 57.8          |              | 438 MB     | 768              | https://huggingface.co/sentence-transformers/all-mpnet-base-v2 |\n",
    "| sentence-transformers/all-MiniLM-L6-v2  | 56.x          |              | 91 MB      | 384              | https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2  |\n",
    "\n",
    "If you would like to, you can also download other models provided by [Hugging Face](https://huggingface.co/spaces/mteb/leaderboard) and compare the results you get from running them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark\n",
    "What we're doing in the code cell below is running a benchmark test on our three chosen models. Essentially, we're downloading the models into our local machines, using them to create embeddings for the string \"Hello World!\" and comparing the speed of each model when it comes to generating embeddings.\n",
    "\n",
    "Don't worry if you see the `TqdmWarning: IProgress not found` warning message, it should not affect your code from running.\n",
    "\n",
    "After running the code cell below, if you look in the `cache` folder of the root directory of your project, you should be able to see that you've downloaded these three embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model=BAAI/bge-small-en-v1.5, embeding_length=384\n",
      "16.5 ms ± 589 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "model=sentence-transformers/all-mpnet-base-v2, embeding_length=768\n",
      "19.3 ms ± 488 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "model=sentence-transformers/all-MiniLM-L6-v2, embeding_length=384\n",
      "9.73 ms ± 189 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import timeit\n",
    "\n",
    "embedding_models = [\n",
    "    'BAAI/bge-small-en-v1.5',\n",
    "    'sentence-transformers/all-mpnet-base-v2',\n",
    "    'sentence-transformers/all-MiniLM-L6-v2',\n",
    "]\n",
    "\n",
    "for model in embedding_models:\n",
    "    embed_model = HuggingFaceEmbedding(model_name=model)\n",
    "    embeddings = embed_model.get_text_embedding(\"Hello World!\")\n",
    "    print(f'model={model}, embeding_length={len(embeddings):,}')\n",
    "    %timeit (embed_model.get_text_embedding(\"Hello World!\"))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [OPTIONAL] Using SentenceTransformers\n",
    "Note that besides using Llama-Index, you can also use other providers like SentenceTransformers to download embedding models; the code cell below demonstrates this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "# embeddings = model.encode(sentences)\n",
    "embeddings = model.encode('a happy dog!')\n",
    "print(model)\n",
    "print (len(embeddings))\n",
    "print(embeddings[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're done with this notebook! Please **head back to the Quest page on StackUp now**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas-1",
   "language": "python",
   "name": "atlas-1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
